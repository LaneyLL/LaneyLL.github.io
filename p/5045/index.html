<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>2D Convolution Optimization | neylin's blog</title><meta name="keywords" content="opt"><meta name="author" content="Li Lin"><meta name="copyright" content="Li Lin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="2D Convolution Optimization 二维卷积优化 本教程概述了如何使用 TVM 在 VTA 设计上有效地映射2d 卷积工作负载。我们建议先学习矩阵乘法Blocking教程。 二维卷积在大多数计算机视觉深层神经网络中占主导地位。在本教程中，我们将演示 TVM 调度优化----将 NCHW 布局中的2d 卷积算子映射到 VTA 上的。我们还介绍了延迟隐藏的概念，它允许我们最大化 V">
<meta property="og:type" content="article">
<meta property="og:title" content="2D Convolution Optimization">
<meta property="og:url" content="https://linl.zone/p/5045/index.html">
<meta property="og:site_name" content="neylin&#39;s blog">
<meta property="og:description" content="2D Convolution Optimization 二维卷积优化 本教程概述了如何使用 TVM 在 VTA 设计上有效地映射2d 卷积工作负载。我们建议先学习矩阵乘法Blocking教程。 二维卷积在大多数计算机视觉深层神经网络中占主导地位。在本教程中，我们将演示 TVM 调度优化----将 NCHW 布局中的2d 卷积算子映射到 VTA 上的。我们还介绍了延迟隐藏的概念，它允许我们最大化 V">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://linl.zone/img/2d.jpg">
<meta property="article:published_time" content="2021-12-29T03:28:52.751Z">
<meta property="article:modified_time" content="2021-12-29T03:32:12.308Z">
<meta property="article:author" content="Li Lin">
<meta property="article:tag" content="opt">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://linl.zone/img/2d.jpg"><link rel="shortcut icon" href="/img/head_new.jpg"><link rel="canonical" href="https://linl.zone/p/5045/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":100,"languages":{"author":"作者: Li Lin","link":"链接: ","source":"来源: neylin's blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '2D Convolution Optimization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-29 11:32:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/head_new.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">31</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> aboutME</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/2d.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">neylin's blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> aboutME</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">2D Convolution Optimization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-29T03:28:52.751Z" title="发表于 2021-12-29 11:28:52">2021-12-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-29T03:32:12.308Z" title="更新于 2021-12-29 11:32:12">2021-12-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/TVM/">TVM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="2D Convolution Optimization"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1><a target="_blank" rel="noopener" href="https://tvm.apache.org/docs/topic/vta/tutorials/optimize/convolution_opt.html?highlight=tiling">2D Convolution Optimization</a> 二维卷积优化</h1>
<p>本教程概述了如何使用 TVM 在 VTA 设计上有效地映射2d 卷积工作负载。我们建议先学习<a target="_blank" rel="noopener" href="https://tvm.apache.org/docs/topic/vta/tutorials/optimize/matrix_multiply_opt.html#vta-mat-mult-opt">矩阵乘法Blocking</a>教程。</p>
<p>二维卷积在大多数计算机视觉深层神经网络中占主导地位。在本教程中，我们将演示 TVM 调度优化----将 NCHW 布局中的2d 卷积算子映射到 VTA 上的。我们还介绍了延迟隐藏的概念，它允许我们最大化 VTA 的计算和内存资源利用率。</p>
<h2 id="RPC-Setup-RPC-设置">RPC Setup RPC 设置</h2>
<p>我们首先对 Pynq 的 FPGA 进行编程，并构建其 RPC 运行时。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"><span class="keyword">import</span> tvm.testing</span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> te</span><br><span class="line"><span class="keyword">import</span> vta</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> rpc</span><br><span class="line"><span class="keyword">from</span> tvm.contrib <span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> vta.testing <span class="keyword">import</span> simulator</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load VTA parameters from the 3rdparty/vta-hw/config/vta_config.json file</span></span><br><span class="line">env = vta.get_env()</span><br><span class="line"></span><br><span class="line"><span class="comment"># We read the Pynq RPC host IP address and port number from the OS environment</span></span><br><span class="line">host = os.environ.get(<span class="string">&quot;VTA_RPC_HOST&quot;</span>, <span class="string">&quot;192.168.2.99&quot;</span>)</span><br><span class="line">port = <span class="built_in">int</span>(os.environ.get(<span class="string">&quot;VTA_RPC_PORT&quot;</span>, <span class="string">&quot;9091&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># We configure both the bitstream and the runtime system on the Pynq</span></span><br><span class="line"><span class="comment"># to match the VTA configuration specified by the vta_config.json file.</span></span><br><span class="line"><span class="keyword">if</span> env.TARGET == <span class="string">&quot;pynq&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make sure that TVM was compiled with RPC=1</span></span><br><span class="line">    <span class="keyword">assert</span> tvm.runtime.enabled(<span class="string">&quot;rpc&quot;</span>)</span><br><span class="line">    remote = rpc.connect(host, port)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reconfigure the JIT runtime</span></span><br><span class="line">    vta.reconfig_runtime(remote)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Program the FPGA with a pre-compiled VTA bitstream.</span></span><br><span class="line">    <span class="comment"># You can program the FPGA with your own custom bitstream</span></span><br><span class="line">    <span class="comment"># by passing the path to the bitstream file instead of None.</span></span><br><span class="line">    vta.program_fpga(remote, bitstream=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In simulation mode, host the RPC server locally.</span></span><br><span class="line"><span class="keyword">elif</span> env.TARGET <span class="keyword">in</span> [<span class="string">&quot;sim&quot;</span>, <span class="string">&quot;tsim&quot;</span>]:</span><br><span class="line">    remote = rpc.LocalSession()</span><br></pre></td></tr></table></figure>
<h2 id="Computation-Declaration-计算声明">Computation Declaration 计算声明</h2>
<p>作为第一步，我们需要描述我们的2d 卷积计算 NCHW 格式。</p>
<p>我们通过批量大小、空间维度、输入通道、输出通道、内核维度、填充尺寸和步幅尺寸来定义二维卷积形状。</p>
<p>我们选择<code>resnet-18</code>体系结构的第<code>9</code>卷积层的形状作为卷积工作负载参数。</p>
<p>我们在2d卷积中增加了额外的算子，这些算子对输出进行移位和剪辑（shifting and clipping ），以模拟定点卷积之后的校正线性激活。下面描述二维卷积层的TVM数据流图:<br>
<img src="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/conv2d_dataflow.png" alt="二维卷积的TVM数据流图"></p>
<p>这个计算量过大，无法同时装载到 VTA 的片上缓冲器中。因此，在调度阶段，我们将依靠计算阻塞策略（computation blocking strategies）将计算分解为可管理的块（chunks）。</p>
<blockquote>
<p><strong>Note</strong></p>
<p>空间填充</p>
<p>注意，我们需要导入 TOPI 库以对输入特征映射张量应用空间填充。空间填充有利于在2d 卷积上下文中进行阻塞（Blocking），因为如果卷积内核窗口大于1，任何给定层的输入特征映射的相同(x，y)空间位置将被多次读取。在 cpu 和 gpu 上，并行化工作时提高内存访问效率的一种方法是空间打包，这需要对数据进行重新布局。VTA 加载 DMA 引擎可以自动插入填充，这样原始的输入特征映射就不必重新打包到内存中。</p>
<p>我们展示了当数据从 DRAM 加载到 VTA 的 SRAM 中时，VTA 对运行空间填充的影响。<br>
<img src="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/padding.png" alt=""></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> topi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2D convolution layer dimensions taken from ResNet-18 architecture</span></span><br><span class="line"><span class="comment"># (9th convolutional layer)</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">height = <span class="number">14</span></span><br><span class="line">width = <span class="number">14</span></span><br><span class="line">in_channels = <span class="number">256</span></span><br><span class="line">out_channels = <span class="number">256</span></span><br><span class="line">kernel_h = <span class="number">3</span></span><br><span class="line">kernel_w = <span class="number">3</span></span><br><span class="line">pad_h = <span class="number">1</span></span><br><span class="line">pad_w = <span class="number">1</span></span><br><span class="line">stride_h = <span class="number">1</span></span><br><span class="line">stride_w = <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> batch_size % env.BATCH == <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> in_channels % env.BLOCK_IN == <span class="number">0</span></span><br><span class="line"><span class="keyword">assert</span> out_channels % env.BLOCK_OUT == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input feature map: (N, IC, H, W, n, ic)</span></span><br><span class="line">data_shape = (</span><br><span class="line">    batch_size // env.BATCH,</span><br><span class="line">    in_channels // env.BLOCK_IN,</span><br><span class="line">    height,</span><br><span class="line">    width,</span><br><span class="line">    env.BATCH,</span><br><span class="line">    env.BLOCK_IN,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Kernel: (OC, IC, H, W, oc, ic)</span></span><br><span class="line">kernel_shape = (</span><br><span class="line">    out_channels // env.BLOCK_OUT,</span><br><span class="line">    in_channels // env.BLOCK_IN,</span><br><span class="line">    kernel_h,</span><br><span class="line">    kernel_w,</span><br><span class="line">    env.BLOCK_OUT,</span><br><span class="line">    env.BLOCK_IN,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Derive output feature map dimensions</span></span><br><span class="line">fout_height = (height + <span class="number">2</span> * pad_h - kernel_h) // stride_h + <span class="number">1</span></span><br><span class="line">fout_width = (width + <span class="number">2</span> * pad_w - kernel_w) // stride_w + <span class="number">1</span></span><br><span class="line"><span class="comment"># Output feature map: (N, OC, H, W, n, oc)</span></span><br><span class="line">output_shape = (</span><br><span class="line">    batch_size // env.BATCH,</span><br><span class="line">    out_channels // env.BLOCK_OUT,</span><br><span class="line">    fout_height,</span><br><span class="line">    fout_width,</span><br><span class="line">    env.BATCH,</span><br><span class="line">    env.BLOCK_OUT,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convolution reduction axes</span></span><br><span class="line">dy = te.reduce_axis((<span class="number">0</span>, kernel_h), name=<span class="string">&quot;dy&quot;</span>)</span><br><span class="line">dx = te.reduce_axis((<span class="number">0</span>, kernel_w), name=<span class="string">&quot;dx&quot;</span>)</span><br><span class="line">ic = te.reduce_axis((<span class="number">0</span>, in_channels // env.BLOCK_IN), name=<span class="string">&quot;ic&quot;</span>)</span><br><span class="line">ic_tns = te.reduce_axis((<span class="number">0</span>, env.BLOCK_IN), name=<span class="string">&quot;ic_tns&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Input placeholder tensors</span></span><br><span class="line">data = te.placeholder(data_shape, name=<span class="string">&quot;data&quot;</span>, dtype=env.inp_dtype)</span><br><span class="line">kernel = te.placeholder(kernel_shape, name=<span class="string">&quot;kernel&quot;</span>, dtype=env.wgt_dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Copy buffers:</span></span><br><span class="line"><span class="comment">#   Apply spatial padding to input feature map</span></span><br><span class="line">data_buf = topi.nn.pad(data, [<span class="number">0</span>, <span class="number">0</span>, pad_h, pad_w, <span class="number">0</span>, <span class="number">0</span>], name=<span class="string">&quot;data_buf&quot;</span>)</span><br><span class="line">kernel_buf = te.compute(kernel_shape, <span class="keyword">lambda</span> *i: kernel(*i), <span class="string">&quot;kernel_buf&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Declare 2D convolution</span></span><br><span class="line">res_conv = te.compute(</span><br><span class="line">    output_shape,</span><br><span class="line">    <span class="keyword">lambda</span> bo, co, i, j, bi, ci: te.<span class="built_in">sum</span>(</span><br><span class="line">        data_buf[bo, ic, i * stride_h + dy, j * stride_w + dx, bi, ic_tns].astype(env.acc_dtype)</span><br><span class="line">        * kernel_buf[co, ic, dy, dx, ci, ic_tns].astype(env.acc_dtype),</span><br><span class="line">        axis=[ic, dy, dx, ic_tns],</span><br><span class="line">    ),</span><br><span class="line">    name=<span class="string">&quot;res_conv&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add shift stage for fix-point normalization</span></span><br><span class="line">res_shr = te.compute(output_shape, <span class="keyword">lambda</span> *i: res_conv(*i) &gt;&gt; <span class="number">8</span>, name=<span class="string">&quot;res_shr&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply clipping between (0, input max value)</span></span><br><span class="line">inp_max = (<span class="number">1</span> &lt;&lt; (env.INP_WIDTH - <span class="number">1</span>)) - <span class="number">1</span></span><br><span class="line">res_max = te.compute(output_shape, <span class="keyword">lambda</span> *i: tvm.te.<span class="built_in">max</span>(res_shr(*i), <span class="number">0</span>), <span class="string">&quot;res_max&quot;</span>)</span><br><span class="line">res_min = te.compute(output_shape, <span class="keyword">lambda</span> *i: tvm.te.<span class="built_in">min</span>(res_max(*i), inp_max), <span class="string">&quot;res_min&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Result Tensor</span></span><br><span class="line">res = te.compute(output_shape, <span class="keyword">lambda</span> *i: res_min(*i).astype(env.inp_dtype), name=<span class="string">&quot;res&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Scheduling-the-Computation-调度计算">Scheduling the Computation 调度计算</h3>
<p>我们将研究一组必要的调度转换，以便以有效的方式将2d 卷积映射到 VTA 上。其中包括:</p>
<ul>
<li>Computation blocking 计算分块</li>
<li>Virtual threading to increase compute utilization 虚拟线程以提高计算利用率</li>
<li>Lowering to VTA hardware intrinsics 降低到 VTA 硬件内部函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create TVM schedule</span></span><br><span class="line">s = te.create_schedule(res.op)</span><br><span class="line"><span class="comment"># Let&#x27;s look at the default TVM schedule</span></span><br><span class="line"><span class="built_in">print</span>(tvm.lower(s, [data, kernel, res], simple_mode=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">@main = primfn(data_1: handle, kernel_1: handle, res_1: handle) -&gt; ()</span><br><span class="line">  attr = &#123;&quot;from_legacy_te_schedule&quot;: True, &quot;global_symbol&quot;: &quot;main&quot;, &quot;tir.noalias&quot;: True&#125;</span><br><span class="line">  buffers = &#123;res: Buffer(res_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),</span><br><span class="line">             data: Buffer(data_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),</span><br><span class="line">             kernel: Buffer(kernel_2: Pointer(int8), int8, [16, 16, 3, 3, 16, 16], [])&#125;</span><br><span class="line">  buffer_map = &#123;data_1: data, kernel_1: kernel, res_1: res&#125; &#123;</span><br><span class="line">  allocate(data_buf: Pointer(global int8), int8, [65536]), storage_scope = global;</span><br><span class="line">  allocate(kernel_buf: Pointer(global int8), int8, [589824]), storage_scope = global;</span><br><span class="line">  allocate(res_conv: Pointer(global int32), int32, [50176]), storage_scope = global &#123;</span><br><span class="line">    for (i1: int32, 0, 16) &#123;</span><br><span class="line">      for (i2: int32, 0, 16) &#123;</span><br><span class="line">        for (i3: int32, 0, 16) &#123;</span><br><span class="line">          for (i5: int32, 0, 16) &#123;</span><br><span class="line">            data_buf[((((i1*4096) + (i2*256)) + (i3*16)) + i5)] = @tir.if_then_else(((((1 &lt;= i2) &amp;&amp; (i2 &lt; 15)) &amp;&amp; (1 &lt;= i3)) &amp;&amp; (i3 &lt; 15)), (int8*)data_2[(((((i1*3136) + (i2*224)) + (i3*16)) + i5) - 240)], 0i8, dtype=int8)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (i0: int32, 0, 16) &#123;</span><br><span class="line">      for (i1_1: int32, 0, 16) &#123;</span><br><span class="line">        for (i2_1: int32, 0, 3) &#123;</span><br><span class="line">          for (i3_1: int32, 0, 3) &#123;</span><br><span class="line">            for (i4: int32, 0, 16) &#123;</span><br><span class="line">              for (i5_1: int32, 0, 16) &#123;</span><br><span class="line">                kernel_buf[((((((i0*36864) + (i1_1*2304)) + (i2_1*768)) + (i3_1*256)) + (i4*16)) + i5_1)] = (int8*)kernel_2[((((((i0*36864) + (i1_1*2304)) + (i2_1*768)) + (i3_1*256)) + (i4*16)) + i5_1)]</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (co: int32, 0, 16) &#123;</span><br><span class="line">      for (i: int32, 0, 14) &#123;</span><br><span class="line">        for (j: int32, 0, 14) &#123;</span><br><span class="line">          for (ci: int32, 0, 16) &#123;</span><br><span class="line">            res_conv[((((co*3136) + (i*224)) + (j*16)) + ci)] = 0</span><br><span class="line">            for (ic: int32, 0, 16) &#123;</span><br><span class="line">              for (dy: int32, 0, 3) &#123;</span><br><span class="line">                for (dx: int32, 0, 3) &#123;</span><br><span class="line">                  for (ic_tns: int32, 0, 16) &#123;</span><br><span class="line">                    res_conv[((((co*3136) + (i*224)) + (j*16)) + ci)] = ((int32*)res_conv[((((co*3136) + (i*224)) + (j*16)) + ci)] + (cast(int32, (int8*)data_buf[((((((ic*4096) + (i*256)) + (dy*256)) + (j*16)) + (dx*16)) + ic_tns)])*cast(int32, (int8*)kernel_buf[((((((co*36864) + (ic*2304)) + (dy*768)) + (dx*256)) + (ci*16)) + ic_tns)])))</span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (i1_2: int32, 0, 16) &#123;</span><br><span class="line">      for (i2_2: int32, 0, 14) &#123;</span><br><span class="line">        for (i3_2: int32, 0, 14) &#123;</span><br><span class="line">          for (i5_2: int32, 0, 16) &#123;</span><br><span class="line">            res_conv[((((i1_2*3136) + (i2_2*224)) + (i3_2*16)) + i5_2)] = @tir.shift_right((int32*)res_conv[((((i1_2*3136) + (i2_2*224)) + (i3_2*16)) + i5_2)], 8, dtype=int32)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (i1_3: int32, 0, 16) &#123;</span><br><span class="line">      for (i2_3: int32, 0, 14) &#123;</span><br><span class="line">        for (i3_3: int32, 0, 14) &#123;</span><br><span class="line">          for (i5_3: int32, 0, 16) &#123;</span><br><span class="line">            res_conv[((((i1_3*3136) + (i2_3*224)) + (i3_3*16)) + i5_3)] = max((int32*)res_conv[((((i1_3*3136) + (i2_3*224)) + (i3_3*16)) + i5_3)], 0)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (i1_4: int32, 0, 16) &#123;</span><br><span class="line">      for (i2_4: int32, 0, 14) &#123;</span><br><span class="line">        for (i3_4: int32, 0, 14) &#123;</span><br><span class="line">          for (i5_4: int32, 0, 16) &#123;</span><br><span class="line">            res_conv[((((i1_4*3136) + (i2_4*224)) + (i3_4*16)) + i5_4)] = min((int32*)res_conv[((((i1_4*3136) + (i2_4*224)) + (i3_4*16)) + i5_4)], 127)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (i1_5: int32, 0, 16) &#123;</span><br><span class="line">      for (i2_5: int32, 0, 14) &#123;</span><br><span class="line">        for (i3_5: int32, 0, 14) &#123;</span><br><span class="line">          for (i5_5: int32, 0, 16) &#123;</span><br><span class="line">            res_2[((((i1_5*3136) + (i2_5*224)) + (i3_5*16)) + i5_5)] = cast(int8, (int32*)res_conv[((((i1_5*3136) + (i2_5*224)) + (i3_5*16)) + i5_5)])</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Blocking-the-Computation-分块计算">Blocking the Computation 分块计算</h3>
<p>默认情况下，2D 卷积对于激活或内核权重来说太大，无法同时适用于 VTA 的片上缓冲区。我们沿着输入通道、输出通道和沿着高的空间维度进行分块。我们不会沿着宽度空间维度应用分块，因为它是 NCHW 布局中最内层的维度(因此为了增加局部性，最好不要沿着最内层的维度应用分块)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let&#x27;s define tiling sizes</span></span><br><span class="line">b_block = <span class="number">1</span> // env.BATCH</span><br><span class="line">oc_block = <span class="number">128</span> // env.BLOCK_OUT</span><br><span class="line">ic_block = <span class="number">16</span> // env.BLOCK_IN</span><br><span class="line">h_block = <span class="number">7</span></span><br><span class="line">w_block = <span class="number">14</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tile the output tensor along the spatial and output channel dimensions</span></span><br><span class="line"><span class="comment"># (since by default we are doing single batch inference, the split along</span></span><br><span class="line"><span class="comment">#  the batch dimension has no effect)</span></span><br><span class="line">b, oc, y, x, b_tns, oc_tns = s[res].op.axis</span><br><span class="line">b_out, b_inn = s[res].split(b, factor=b_block)</span><br><span class="line">oc_out, oc_inn = s[res].split(oc, factor=oc_block)</span><br><span class="line">y_out, y_inn = s[res].split(y, factor=h_block)</span><br><span class="line">x_out, x_inn = s[res].split(x, factor=w_block)</span><br><span class="line">s[res].reorder(b_out, oc_out, y_out, x_out, b_inn, oc_inn, y_inn, x_inn, b_tns, oc_tns)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move intermediate computation into each output compute tile</span></span><br><span class="line">s[res_conv].compute_at(s[res], x_out)</span><br><span class="line">s[res_shr].compute_at(s[res], x_out)</span><br><span class="line">s[res_max].compute_at(s[res], x_out)</span><br><span class="line">s[res_min].compute_at(s[res], x_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply additional loop split along reduction axis (input channel)</span></span><br><span class="line">b_inn, oc_inn, y_inn, x_inn, b_tns, oc_tns = s[res_conv].op.axis</span><br><span class="line">ic_out, ic_inn = s[res_conv].split(ic, factor=ic_block)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reorder axes.</span></span><br><span class="line"><span class="comment"># 1) Group the VTA tensor axes in the inner most position: b_tns, oc_tns, ic_tns</span></span><br><span class="line"><span class="comment">#    to allow TVM to tensorize.</span></span><br><span class="line"><span class="comment"># 2) We move the ic_out axis all the way out of the convolution loop to block</span></span><br><span class="line"><span class="comment">#    along the reduction axis.</span></span><br><span class="line"><span class="comment"># 3) Now we re-order the block axes: b_inn, oc_inn, y_inn, x_inn, ic_inn, dy, dx.</span></span><br><span class="line"><span class="comment">#    VTA runtime/hardware requires us to write to a different output feature map</span></span><br><span class="line"><span class="comment">#    location for every VTA tensor operation.</span></span><br><span class="line"><span class="comment">#    This restriction requires us to order one of oc_inn, y_inn or x_inn right</span></span><br><span class="line"><span class="comment">#    before b_tns, since they all affect output feature map indexing.</span></span><br><span class="line"><span class="comment">#    Therefore, we choose to bring x_inn inside as shown below.</span></span><br><span class="line">s[res_conv].reorder(ic_out, b_inn, oc_inn, y_inn, ic_inn, dy, dx, x_inn, b_tns, oc_tns, ic_tns)</span><br></pre></td></tr></table></figure>
<h3 id="Virtual-Threading-虚拟线程">Virtual Threading 虚拟线程</h3>
<p>虚拟线程是 VTA 硬件设计中增加任务级流水线并行性的一种机制。换句话说，它通过隐藏内存访问延迟来提高计算资源利用率。</p>
<p>在下面的实现中，虚拟线程将工作分布在沿输出通道轴分离的两个线程上。我们在下图中展示了在计算二维卷积时工作是如何被分割的。</p>
<p><img src="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/virtual_threading.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VTA only supports 2 virtual threads</span></span><br><span class="line">v_threads = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform virtual thread split along output channel outer axis</span></span><br><span class="line">_, tx = s[res].split(oc_out, factor=v_threads)</span><br><span class="line">s[res].reorder(tx, b_out)</span><br><span class="line">s[res].bind(tx, te.thread_axis(<span class="string">&quot;cthread&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let&#x27;s look at the current TVM schedule after blocking and virtual threading</span></span><br><span class="line"><span class="built_in">print</span>(tvm.lower(s, [data, kernel, res], simple_mode=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">@main = primfn(data_1: handle, kernel_1: handle, res_1: handle) -&gt; ()</span><br><span class="line">  attr = &#123;&quot;from_legacy_te_schedule&quot;: True, &quot;global_symbol&quot;: &quot;main&quot;, &quot;tir.noalias&quot;: True&#125;</span><br><span class="line">  buffers = &#123;res: Buffer(res_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),</span><br><span class="line">             data: Buffer(data_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),</span><br><span class="line">             kernel: Buffer(kernel_2: Pointer(int8), int8, [16, 16, 3, 3, 16, 16], [])&#125;</span><br><span class="line">  buffer_map = &#123;data_1: data, kernel_1: kernel, res_1: res&#125; &#123;</span><br><span class="line">  allocate(data_buf: Pointer(global int8), int8, [65536]), storage_scope = global;</span><br><span class="line">  allocate(kernel_buf: Pointer(global int8), int8, [589824]), storage_scope = global;</span><br><span class="line">  allocate(res_conv: Pointer(global int32), int32, [25088]), storage_scope = global &#123;</span><br><span class="line">    for (i1: int32, 0, 16) &#123;</span><br><span class="line">      for (i2: int32, 0, 16) &#123;</span><br><span class="line">        for (i3: int32, 0, 16) &#123;</span><br><span class="line">          for (i5: int32, 0, 16) &#123;</span><br><span class="line">            data_buf[((((i1*4096) + (i2*256)) + (i3*16)) + i5)] = @tir.if_then_else(((((1 &lt;= i2) &amp;&amp; (i2 &lt; 15)) &amp;&amp; (1 &lt;= i3)) &amp;&amp; (i3 &lt; 15)), (int8*)data_2[(((((i1*3136) + (i2*224)) + (i3*16)) + i5) - 240)], 0i8, dtype=int8)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (i0: int32, 0, 16) &#123;</span><br><span class="line">      for (i1_1: int32, 0, 16) &#123;</span><br><span class="line">        for (i2_1: int32, 0, 3) &#123;</span><br><span class="line">          for (i3_1: int32, 0, 3) &#123;</span><br><span class="line">            for (i4: int32, 0, 16) &#123;</span><br><span class="line">              for (i5_1: int32, 0, 16) &#123;</span><br><span class="line">                kernel_buf[((((((i0*36864) + (i1_1*2304)) + (i2_1*768)) + (i3_1*256)) + (i4*16)) + i5_1)] = (int8*)kernel_2[((((((i0*36864) + (i1_1*2304)) + (i2_1*768)) + (i3_1*256)) + (i4*16)) + i5_1)]</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (i2.outer: int32, 0, 2) &#123;</span><br><span class="line">      for (co.init: int32, 0, 8) &#123;</span><br><span class="line">        for (i.init: int32, 0, 7) &#123;</span><br><span class="line">          for (j.init: int32, 0, 14) &#123;</span><br><span class="line">            for (ci.init: int32, 0, 16) &#123;</span><br><span class="line">              res_conv[((((co.init*1568) + (i.init*224)) + (j.init*16)) + ci.init)] = 0</span><br><span class="line">              res_conv[(((((co.init*1568) + (i.init*224)) + (j.init*16)) + ci.init) + 12544)] = 0</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      for (ic.outer: int32, 0, 16) &#123;</span><br><span class="line">        for (co: int32, 0, 8) &#123;</span><br><span class="line">          for (i: int32, 0, 7) &#123;</span><br><span class="line">            for (dy: int32, 0, 3) &#123;</span><br><span class="line">              for (dx: int32, 0, 3) &#123;</span><br><span class="line">                for (j: int32, 0, 14) &#123;</span><br><span class="line">                  for (ci: int32, 0, 16) &#123;</span><br><span class="line">                    for (ic_tns: int32, 0, 16) &#123;</span><br><span class="line">                      res_conv[((((co*1568) + (i*224)) + (j*16)) + ci)] = ((int32*)res_conv[((((co*1568) + (i*224)) + (j*16)) + ci)] + (cast(int32, (int8*)data_buf[(((((((ic.outer*4096) + (i2.outer*1792)) + (i*256)) + (dy*256)) + (j*16)) + (dx*16)) + ic_tns)])*cast(int32, (int8*)kernel_buf[((((((co*36864) + (ic.outer*2304)) + (dy*768)) + (dx*256)) + (ci*16)) + ic_tns)])))</span><br><span class="line">                      res_conv[(((((co*1568) + (i*224)) + (j*16)) + ci) + 12544)] = ((int32*)res_conv[(((((co*1568) + (i*224)) + (j*16)) + ci) + 12544)] + (cast(int32, (int8*)data_buf[(((((((ic.outer*4096) + (i2.outer*1792)) + (i*256)) + (dy*256)) + (j*16)) + (dx*16)) + ic_tns)])*cast(int32, (int8*)kernel_buf[(((((((co*36864) + (ic.outer*2304)) + (dy*768)) + (dx*256)) + (ci*16)) + ic_tns) + 294912)])))</span><br><span class="line">                    &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      for (i1_2: int32, 0, 8) &#123;</span><br><span class="line">        for (i2_2: int32, 0, 7) &#123;</span><br><span class="line">          for (i3_2: int32, 0, 14) &#123;</span><br><span class="line">            for (i5_2: int32, 0, 16) &#123;</span><br><span class="line">              res_conv[((((i1_2*1568) + (i2_2*224)) + (i3_2*16)) + i5_2)] = @tir.shift_right((int32*)res_conv[((((i1_2*1568) + (i2_2*224)) + (i3_2*16)) + i5_2)], 8, dtype=int32)</span><br><span class="line">              res_conv[(((((i1_2*1568) + (i2_2*224)) + (i3_2*16)) + i5_2) + 12544)] = @tir.shift_right((int32*)res_conv[(((((i1_2*1568) + (i2_2*224)) + (i3_2*16)) + i5_2) + 12544)], 8, dtype=int32)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      for (i1_3: int32, 0, 8) &#123;</span><br><span class="line">        for (i2_3: int32, 0, 7) &#123;</span><br><span class="line">          for (i3_3: int32, 0, 14) &#123;</span><br><span class="line">            for (i5_3: int32, 0, 16) &#123;</span><br><span class="line">              res_conv[((((i1_3*1568) + (i2_3*224)) + (i3_3*16)) + i5_3)] = max((int32*)res_conv[((((i1_3*1568) + (i2_3*224)) + (i3_3*16)) + i5_3)], 0)</span><br><span class="line">              res_conv[(((((i1_3*1568) + (i2_3*224)) + (i3_3*16)) + i5_3) + 12544)] = max((int32*)res_conv[(((((i1_3*1568) + (i2_3*224)) + (i3_3*16)) + i5_3) + 12544)], 0)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      for (i1_4: int32, 0, 8) &#123;</span><br><span class="line">        for (i2_4: int32, 0, 7) &#123;</span><br><span class="line">          for (i3_4: int32, 0, 14) &#123;</span><br><span class="line">            for (i5_4: int32, 0, 16) &#123;</span><br><span class="line">              res_conv[((((i1_4*1568) + (i2_4*224)) + (i3_4*16)) + i5_4)] = min((int32*)res_conv[((((i1_4*1568) + (i2_4*224)) + (i3_4*16)) + i5_4)], 127)</span><br><span class="line">              res_conv[(((((i1_4*1568) + (i2_4*224)) + (i3_4*16)) + i5_4) + 12544)] = min((int32*)res_conv[(((((i1_4*1568) + (i2_4*224)) + (i3_4*16)) + i5_4) + 12544)], 127)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      for (i1.inner: int32, 0, 8) &#123;</span><br><span class="line">        for (i2.inner: int32, 0, 7) &#123;</span><br><span class="line">          for (i3.inner: int32, 0, 14) &#123;</span><br><span class="line">            for (i5_5: int32, 0, 16) &#123;</span><br><span class="line">              res_2[(((((i1.inner*3136) + (i2.outer*1568)) + (i2.inner*224)) + (i3.inner*16)) + i5_5)] = cast(int8, (int32*)res_conv[((((i1.inner*1568) + (i2.inner*224)) + (i3.inner*16)) + i5_5)])</span><br><span class="line">              res_2[((((((i1.inner*3136) + (i2.outer*1568)) + (i2.inner*224)) + (i3.inner*16)) + i5_5) + 25088)] = cast(int8, (int32*)res_conv[(((((i1.inner*1568) + (i2.inner*224)) + (i3.inner*16)) + i5_5) + 12544)])</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Lowering-Copies-to-DMA-Transfers-降低-DMA-传输的副本">Lowering Copies to DMA Transfers 降低 DMA 传输的副本</h3>
<p>接下来，我们将缓冲区范围设置为相应的片上 VTA SRAM 缓冲区。我们将负载循环移动到2d 卷积计算循环，以分阶段存储器负载，使他们适合在片上 SRAM 缓冲器。最后，我们使用 DMA 复制杂注来注释装载/存储循环外轴，以便在 VTA 上执行批量内存传输。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set scope of SRAM buffers</span></span><br><span class="line">s[data_buf].set_scope(env.inp_scope)</span><br><span class="line">s[kernel_buf].set_scope(env.wgt_scope)</span><br><span class="line">s[res_conv].set_scope(env.acc_scope)</span><br><span class="line">s[res_shr].set_scope(env.acc_scope)</span><br><span class="line">s[res_min].set_scope(env.acc_scope)</span><br><span class="line">s[res_max].set_scope(env.acc_scope)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Block data and kernel cache reads</span></span><br><span class="line">s[data_buf].compute_at(s[res_conv], ic_out)</span><br><span class="line">s[kernel_buf].compute_at(s[res_conv], ic_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use DMA copy pragma on DRAM-&gt;SRAM operations</span></span><br><span class="line">s[data_buf].pragma(s[data_buf].op.axis[<span class="number">0</span>], env.dma_copy)</span><br><span class="line">s[kernel_buf].pragma(s[kernel_buf].op.axis[<span class="number">0</span>], env.dma_copy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use DMA copy pragma on SRAM-&gt;DRAM operation in each result block</span></span><br><span class="line"><span class="comment"># (this implies that these copies should be performed along b_inn,</span></span><br><span class="line"><span class="comment"># or result axis 4)</span></span><br><span class="line">s[res].pragma(s[res].op.axis[<span class="number">4</span>], env.dma_copy)</span><br></pre></td></tr></table></figure>
<h3 id="Lowering-Computation-to-VTA-Compute-Intrinsics-VTA-计算内部函数的降阶计算">Lowering Computation to VTA Compute Intrinsics VTA 计算内部函数的降阶计算</h3>
<p>最后一个阶段是通过将2d 卷积映射到张量内部函数，并将移位映射到矢量 ALU，从而将计算循环降低到 VTA 硬件内部函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply tensorization over the batch tensor tile axis</span></span><br><span class="line">s[res_conv].tensorize(b_tns, env.gemm)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add an ALU pragma over the shift and clipping operations</span></span><br><span class="line">s[res_shr].pragma(s[res_shr].op.axis[<span class="number">0</span>], env.alu)</span><br><span class="line">s[res_min].pragma(s[res_min].op.axis[<span class="number">0</span>], env.alu)</span><br><span class="line">s[res_max].pragma(s[res_max].op.axis[<span class="number">0</span>], env.alu)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let&#x27;s look at the final lowered TVM schedule after lowering memory</span></span><br><span class="line"><span class="comment"># loads/stores down to DMA copy intrinsics, and the computation down to</span></span><br><span class="line"><span class="comment"># VTA compute intrinsics.</span></span><br><span class="line"><span class="built_in">print</span>(vta.lower(s, [data, kernel, res], simple_mode=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">@main = primfn(data_1: handle, kernel_1: handle, res_1: handle) -&gt; ()</span><br><span class="line">  attr = &#123;&quot;from_legacy_te_schedule&quot;: True, &quot;global_symbol&quot;: &quot;main&quot;, &quot;tir.noalias&quot;: True&#125;</span><br><span class="line">  buffers = &#123;res: Buffer(res_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),</span><br><span class="line">             data: Buffer(data_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),</span><br><span class="line">             kernel: Buffer(kernel_2: Pointer(int8), int8, [16, 16, 3, 3, 16, 16], [])&#125;</span><br><span class="line">  buffer_map = &#123;data_1: data, kernel_1: kernel, res_1: res&#125; &#123;</span><br><span class="line">  @tir.vta.coproc_dep_push(3, 2, dtype=int32)</span><br><span class="line">  @tir.vta.coproc_dep_push(3, 2, dtype=int32)</span><br><span class="line">  for (i2.outer: int32, 0, 2) &#123;</span><br><span class="line">    for (cthread.s: int32, 0, 2) &#123;</span><br><span class="line">      attr [IterVar(vta: int32, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_scope&quot; = 2 &#123;</span><br><span class="line">        @tir.vta.coproc_dep_pop(3, 2, dtype=int32)</span><br><span class="line">        attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_uop_scope&quot; = &quot;VTAPushGEMMOp&quot; &#123;</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopBegin&quot;, 8, 98, 0, 0, dtype=int32)</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopBegin&quot;, 7, 14, 0, 0, dtype=int32)</span><br><span class="line">          for (j.init: int32, 0, 14) &#123;</span><br><span class="line">            @tir.vta.uop_push(0, 1, ((cthread.s*784) + j.init), 0, 0, 0, 0, 0, dtype=int32)</span><br><span class="line">          &#125;</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopEnd&quot;, dtype=int32)</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopEnd&quot;, dtype=int32)</span><br><span class="line">        &#125;</span><br><span class="line">        @tir.vta.coproc_dep_push(2, 1, dtype=int32)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (ic.outer: int32, 0, 16) &#123;</span><br><span class="line">      attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_scope&quot; = 1 &#123;</span><br><span class="line">        @tir.vta.coproc_dep_pop(2, 1, dtype=int32)</span><br><span class="line">        @tir.call_extern(&quot;VTALoadBuffer2D&quot;, @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), data_2, ((((ic.outer*196) + (i2.outer*98)) + (max((1 - (i2.outer*7)), 0)*14)) - 14), 14, ((9 - max((1 - (i2.outer*7)), 0)) - max(((i2.outer*7) - 6), 0)), 14, 1, max((1 - (i2.outer*7)), 0), 1, max(((i2.outer*7) - 6), 0), 0, 2, dtype=int32)</span><br><span class="line">        @tir.call_extern(&quot;VTALoadBuffer2D&quot;, @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), kernel_2, (ic.outer*9), 9, 8, 144, 0, 0, 0, 0, 0, 1, dtype=int32)</span><br><span class="line">        @tir.vta.coproc_dep_push(1, 2, dtype=int32)</span><br><span class="line">      &#125;</span><br><span class="line">      attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_scope&quot; = 1 &#123;</span><br><span class="line">        @tir.vta.coproc_dep_pop(2, 1, dtype=int32)</span><br><span class="line">        @tir.call_extern(&quot;VTALoadBuffer2D&quot;, @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), data_2, ((((ic.outer*196) + (i2.outer*98)) + (max((1 - (i2.outer*7)), 0)*14)) - 14), 14, ((9 - max((1 - (i2.outer*7)), 0)) - max(((i2.outer*7) - 6), 0)), 14, 1, max((1 - (i2.outer*7)), 0), 1, max(((i2.outer*7) - 6), 0), 144, 2, dtype=int32)</span><br><span class="line">        @tir.call_extern(&quot;VTALoadBuffer2D&quot;, @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), kernel_2, ((ic.outer*9) + 1152), 9, 8, 144, 0, 0, 0, 0, 72, 1, dtype=int32)</span><br><span class="line">        @tir.vta.coproc_dep_push(1, 2, dtype=int32)</span><br><span class="line">      &#125;</span><br><span class="line">      for (cthread.s_1: int32, 0, 2) &#123;</span><br><span class="line">        attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_scope&quot; = 2 &#123;</span><br><span class="line">          @tir.vta.coproc_dep_pop(1, 2, dtype=int32)</span><br><span class="line">          attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_uop_scope&quot; = &quot;VTAPushGEMMOp&quot; &#123;</span><br><span class="line">            @tir.call_extern(&quot;VTAUopLoopBegin&quot;, 8, 98, 0, 9, dtype=int32)</span><br><span class="line">            @tir.call_extern(&quot;VTAUopLoopBegin&quot;, 7, 14, 16, 0, dtype=int32)</span><br><span class="line">            for (dy: int32, 0, 3) &#123;</span><br><span class="line">              for (dx: int32, 0, 3) &#123;</span><br><span class="line">                for (j: int32, 0, 14) &#123;</span><br><span class="line">                  @tir.vta.uop_push(0, 0, ((cthread.s_1*784) + j), ((((cthread.s_1*144) + (dy*16)) + j) + dx), (((cthread.s_1*72) + (dy*3)) + dx), 0, 0, 0, dtype=int32)</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            @tir.call_extern(&quot;VTAUopLoopEnd&quot;, dtype=int32)</span><br><span class="line">            @tir.call_extern(&quot;VTAUopLoopEnd&quot;, dtype=int32)</span><br><span class="line">          &#125;</span><br><span class="line">          @tir.vta.coproc_dep_push(2, 1, dtype=int32)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    @tir.vta.coproc_dep_pop(2, 1, dtype=int32)</span><br><span class="line">    @tir.vta.coproc_dep_pop(2, 1, dtype=int32)</span><br><span class="line">    for (cthread.s_2: int32, 0, 2) &#123;</span><br><span class="line">      attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_scope&quot; = 2 &#123;</span><br><span class="line">        attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_uop_scope&quot; = &quot;VTAPushALUOp&quot; &#123;</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopBegin&quot;, 784, 1, 1, 0, dtype=int32)</span><br><span class="line">          @tir.vta.uop_push(1, 0, (cthread.s_2*784), (cthread.s_2*784), 0, 3, 1, 8, dtype=int32)</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopEnd&quot;, dtype=int32)</span><br><span class="line">        &#125;</span><br><span class="line">        attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_uop_scope&quot; = &quot;VTAPushALUOp&quot; &#123;</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopBegin&quot;, 784, 1, 1, 0, dtype=int32)</span><br><span class="line">          @tir.vta.uop_push(1, 0, (cthread.s_2*784), (cthread.s_2*784), 0, 1, 1, 0, dtype=int32)</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopEnd&quot;, dtype=int32)</span><br><span class="line">        &#125;</span><br><span class="line">        attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_uop_scope&quot; = &quot;VTAPushALUOp&quot; &#123;</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopBegin&quot;, 784, 1, 1, 0, dtype=int32)</span><br><span class="line">          @tir.vta.uop_push(1, 0, (cthread.s_2*784), (cthread.s_2*784), 0, 0, 1, 127, dtype=int32)</span><br><span class="line">          @tir.call_extern(&quot;VTAUopLoopEnd&quot;, dtype=int32)</span><br><span class="line">        &#125;</span><br><span class="line">        @tir.vta.coproc_dep_push(2, 3, dtype=int32)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (cthread.s_3: int32, 0, 2) &#123;</span><br><span class="line">      attr [IterVar(vta, (nullptr), &quot;ThreadIndex&quot;, &quot;vta&quot;)] &quot;coproc_scope&quot; = 3 &#123;</span><br><span class="line">        @tir.vta.coproc_dep_pop(2, 3, dtype=int32)</span><br><span class="line">        for (i1.inner: int32, 0, 8) &#123;</span><br><span class="line">          for (i2.inner: int32, 0, 7) &#123;</span><br><span class="line">            for (i3.inner: int32, 0, 14) &#123;</span><br><span class="line">              @tir.call_extern(&quot;VTAStoreBuffer2D&quot;, @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), ((((cthread.s_3*784) + (i1.inner*98)) + (i2.inner*14)) + i3.inner), 4, res_2, (((((cthread.s_3*1568) + (i1.inner*196)) + (i2.outer*98)) + (i2.inner*14)) + i3.inner), 1, 1, 1, dtype=int32)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        @tir.vta.coproc_dep_push(3, 2, dtype=int32)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  @tir.vta.coproc_dep_pop(3, 2, dtype=int32)</span><br><span class="line">  @tir.vta.coproc_dep_pop(3, 2, dtype=int32)</span><br><span class="line">  @tir.vta.coproc_sync(, dtype=int32)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="TVM-Compilation-and-Verification-TVM-的编译与验证">TVM Compilation and Verification TVM 的编译与验证</h2>
<p>在指定了调度之后，我们可以将它编译成 TVM 函数。我们保存这个模块，以便通过 RPC 发送它。我们运行该函数并在 numpy 实现中验证它，以确保正确性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This library facilitates 2D convolution testing</span></span><br><span class="line"><span class="keyword">from</span> tvm.topi.testing <span class="keyword">import</span> conv2d_nchw_python</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compile the TVM module</span></span><br><span class="line">my_conv = vta.build(</span><br><span class="line">    s, [data, kernel, res], tvm.target.Target(<span class="string">&quot;ext_dev&quot;</span>, host=env.target_host), name=<span class="string">&quot;my_conv&quot;</span></span><br><span class="line">)</span><br><span class="line">temp = utils.tempdir()</span><br><span class="line">my_conv.save(temp.relpath(<span class="string">&quot;conv2d.o&quot;</span>))</span><br><span class="line">remote.upload(temp.relpath(<span class="string">&quot;conv2d.o&quot;</span>))</span><br><span class="line">f = remote.load_module(<span class="string">&quot;conv2d.o&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the remote device context</span></span><br><span class="line">ctx = remote.ext_dev(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the data and kernel arrays randomly in the int range</span></span><br><span class="line"><span class="comment"># of (-128, 128] in NCHW layout</span></span><br><span class="line">data_np = np.random.randint(-<span class="number">128</span>, <span class="number">128</span>, size=(batch_size, in_channels, height, width)).astype(</span><br><span class="line">    data.dtype</span><br><span class="line">)</span><br><span class="line">kernel_np = np.random.randint(</span><br><span class="line">    -<span class="number">128</span>, <span class="number">128</span>, size=(out_channels, in_channels, kernel_h, kernel_w)</span><br><span class="line">).astype(kernel.dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply packing to the data and kernel arrays from a 2D NCHW</span></span><br><span class="line"><span class="comment"># to a 4D NCHWnc packed layout</span></span><br><span class="line">data_packed = data_np.reshape(</span><br><span class="line">    batch_size // env.BATCH, env.BATCH, in_channels // env.BLOCK_IN, env.BLOCK_IN, height, width</span><br><span class="line">).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">kernel_packed = kernel_np.reshape(</span><br><span class="line">    out_channels // env.BLOCK_OUT,</span><br><span class="line">    env.BLOCK_OUT,</span><br><span class="line">    in_channels // env.BLOCK_IN,</span><br><span class="line">    env.BLOCK_IN,</span><br><span class="line">    kernel_h,</span><br><span class="line">    kernel_w,</span><br><span class="line">).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Format the input/output arrays with tvm.nd.array to the DLPack standard</span></span><br><span class="line">data_nd = tvm.nd.array(data_packed, ctx)</span><br><span class="line">kernel_nd = tvm.nd.array(kernel_packed, ctx)</span><br><span class="line">res_nd = tvm.nd.array(np.zeros(output_shape).astype(res.dtype), ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clear stats</span></span><br><span class="line"><span class="keyword">if</span> env.TARGET <span class="keyword">in</span> [<span class="string">&quot;sim&quot;</span>, <span class="string">&quot;tsim&quot;</span>]:</span><br><span class="line">    simulator.clear_stats()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Invoke the module to perform the computation</span></span><br><span class="line">f(data_nd, kernel_nd, res_nd)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify against numpy implementation</span></span><br><span class="line">res_ref = conv2d_nchw_python(</span><br><span class="line">    data_np.astype(env.acc_dtype),</span><br><span class="line">    kernel_np.astype(env.acc_dtype),</span><br><span class="line">    (stride_h, stride_w),</span><br><span class="line">    (pad_h, pad_w),</span><br><span class="line">).astype(env.acc_dtype)</span><br><span class="line">res_ref = res_ref &gt;&gt; env.INP_WIDTH</span><br><span class="line">res_ref = np.clip(res_ref, <span class="number">0</span>, inp_max)</span><br><span class="line">res_ref = res_ref.astype(res.dtype)</span><br><span class="line">res_ref = res_ref.reshape(</span><br><span class="line">    (</span><br><span class="line">        batch_size // env.BATCH,</span><br><span class="line">        env.BATCH,</span><br><span class="line">        out_channels // env.BLOCK_OUT,</span><br><span class="line">        env.BLOCK_OUT,</span><br><span class="line">        fout_height,</span><br><span class="line">        fout_width,</span><br><span class="line">    )</span><br><span class="line">).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">tvm.testing.assert_allclose(res_ref, res_nd.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print stats</span></span><br><span class="line"><span class="keyword">if</span> env.TARGET <span class="keyword">in</span> [<span class="string">&quot;sim&quot;</span>, <span class="string">&quot;tsim&quot;</span>]:</span><br><span class="line">    sim_stats = simulator.stats()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Execution statistics:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> sim_stats.items():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t&#123;:&lt;16&#125;: &#123;:&gt;16&#125;&quot;</span>.<span class="built_in">format</span>(k, v))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Successful 2D convolution test!&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Execution statistics:</span><br><span class="line">        inp_load_nbytes :           114688</span><br><span class="line">        wgt_load_nbytes :          1179648</span><br><span class="line">        acc_load_nbytes :                0</span><br><span class="line">        uop_load_nbytes :             1144</span><br><span class="line">        out_store_nbytes:            50176</span><br><span class="line">        gemm_counter    :           451584</span><br><span class="line">        alu_counter     :             9408</span><br><span class="line">Successful 2D convolution test!</span><br></pre></td></tr></table></figure>
<h2 id="Summary-结论">Summary 结论</h2>
<p>本教程演示了如何使用 TVM 调度原语降低硬件加速器内部函数的二维卷积，利用特定于硬件的优化，如隐藏在虚拟线程中的延迟。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">@main = primfn(A_1: handle, B_1: handle) -&gt; ()</span><br><span class="line">  attr = &#123;&quot;from_legacy_te_schedule&quot;: True, &quot;global_symbol&quot;: &quot;main&quot;, &quot;tir.noalias&quot;: True&#125;</span><br><span class="line">  buffers = &#123;B: Buffer(B_2: Pointer(float32), float32, [m: int32, n: int32], [stride: int32, stride_1: int32], type=&quot;auto&quot;),</span><br><span class="line">             A: Buffer(A_2: Pointer(float32), float32, [m, n], [stride_2: int32, stride_3: int32], type=&quot;auto&quot;)&#125;</span><br><span class="line">  buffer_map = &#123;A_1: A, B_1: B&#125; &#123;</span><br><span class="line">  for (i.outer: int32, 0, floordiv((m + 9), 10)) &#123;</span><br><span class="line">    for (j.outer: int32, 0, floordiv((n + 4), 5)) &#123;</span><br><span class="line">      for (i.inner: int32, 0, 10) &#123;</span><br><span class="line">        if @tir.likely((((i.outer*10) + i.inner) &lt; m), dtype=bool) &#123;</span><br><span class="line">          for (j.inner: int32, 0, 5) &#123;</span><br><span class="line">            if @tir.likely((((j.outer*5) + j.inner) &lt; n), dtype=bool) &#123;</span><br><span class="line">              B_2[((((i.outer*10) + i.inner)*stride) + (((j.outer*5) + j.inner)*stride_1))] = (float32*)A_2[((((i.outer*10) + i.inner)*stride_2) + (((j.outer*5) + j.inner)*stride_3))]</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Li Lin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://linl.zone/p/5045/">https://linl.zone/p/5045/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://linl.zone" target="_blank">neylin's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/opt/">opt</a></div><div class="post_share"><div class="social-share" data-image="/img/2d.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/p/95/"><img class="prev-cover" src="/img/lc1229.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">LeetCode1229</div></div></a></div><div class="next-post pull-right"><a href="/p/23534/"><img class="next-cover" src="/img/lc0102.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LeetCode0102</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="utterances-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/head_new.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Li Lin</div><div class="author-info__description">tech & job</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">31</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/LaneyLL"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/LaneyLL" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:136122596@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">neyLin's studio</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">2D Convolution Optimization 二维卷积优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RPC-Setup-RPC-%E8%AE%BE%E7%BD%AE"><span class="toc-text">RPC Setup RPC 设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Computation-Declaration-%E8%AE%A1%E7%AE%97%E5%A3%B0%E6%98%8E"><span class="toc-text">Computation Declaration 计算声明</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scheduling-the-Computation-%E8%B0%83%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-text">Scheduling the Computation 调度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Blocking-the-Computation-%E5%88%86%E5%9D%97%E8%AE%A1%E7%AE%97"><span class="toc-text">Blocking the Computation 分块计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Virtual-Threading-%E8%99%9A%E6%8B%9F%E7%BA%BF%E7%A8%8B"><span class="toc-text">Virtual Threading 虚拟线程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lowering-Copies-to-DMA-Transfers-%E9%99%8D%E4%BD%8E-DMA-%E4%BC%A0%E8%BE%93%E7%9A%84%E5%89%AF%E6%9C%AC"><span class="toc-text">Lowering Copies to DMA Transfers 降低 DMA 传输的副本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lowering-Computation-to-VTA-Compute-Intrinsics-VTA-%E8%AE%A1%E7%AE%97%E5%86%85%E9%83%A8%E5%87%BD%E6%95%B0%E7%9A%84%E9%99%8D%E9%98%B6%E8%AE%A1%E7%AE%97"><span class="toc-text">Lowering Computation to VTA Compute Intrinsics VTA 计算内部函数的降阶计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TVM-Compilation-and-Verification-TVM-%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E9%AA%8C%E8%AF%81"><span class="toc-text">TVM Compilation and Verification TVM 的编译与验证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary-%E7%BB%93%E8%AE%BA"><span class="toc-text">Summary 结论</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/p/45050/" title="零钱兑换+两数相加+平衡二叉树"><img src="/img/0107.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="零钱兑换+两数相加+平衡二叉树"/></a><div class="content"><a class="title" href="/p/45050/" title="零钱兑换+两数相加+平衡二叉树">零钱兑换+两数相加+平衡二叉树</a><time datetime="2022-01-17T06:08:39.745Z" title="发表于 2022-01-17 14:08:39">2022-01-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/52887/" title="学习笔记"><img src="/img/0107.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="学习笔记"/></a><div class="content"><a class="title" href="/p/52887/" title="学习笔记">学习笔记</a><time datetime="2022-01-12T07:59:00.138Z" title="发表于 2022-01-12 15:59:00">2022-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/14354/" title="有效括号字符串"><img src="/img/lc1227.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="有效括号字符串"/></a><div class="content"><a class="title" href="/p/14354/" title="有效括号字符串">有效括号字符串</a><time datetime="2022-01-12T05:45:08.231Z" title="发表于 2022-01-12 13:45:08">2022-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/59829/" title="socket编程"><img src="/img/lc1219.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="socket编程"/></a><div class="content"><a class="title" href="/p/59829/" title="socket编程">socket编程</a><time datetime="2022-01-12T01:33:02.511Z" title="发表于 2022-01-12 09:33:02">2022-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/30013/" title="回文子串+单词翻转"><img src="/img/opt_conv.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="回文子串+单词翻转"/></a><div class="content"><a class="title" href="/p/30013/" title="回文子串+单词翻转">回文子串+单词翻转</a><time datetime="2022-01-11T08:28:05.350Z" title="发表于 2022-01-11 16:28:05">2022-01-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By Li Lin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://linl.zone/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadUtterances () {
  let ele = document.createElement('script')
  ele.setAttribute('id', 'utterances_comment')
  ele.setAttribute('src', 'https://utteranc.es/client.js')
  ele.setAttribute('repo', 'LaneyLL/gittalk')
  ele.setAttribute('issue-term', 'pathname')
  let nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'photon-dark' : 'github-light'
  ele.setAttribute('theme', nowTheme)
  ele.setAttribute('crossorigin', 'anonymous')
  ele.setAttribute('async', 'true')
  document.getElementById('utterances-wrap').insertAdjacentElement('afterbegin',ele)
}

function utterancesTheme () {
  if (document.querySelector('.utterances-frame')) {
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'photon-dark' : 'github-light'
    const message = {
      type: 'set-theme',
      theme: theme
    };
    const iframe = document.querySelector('.utterances-frame');
    iframe.contentWindow.postMessage(message, 'https://utteranc.es');
  }
}

if ('Utterances' === 'Utterances' || !true) {
  if (true) btf.loadComment(document.getElementById('utterances-wrap'), loadUtterances)
  else loadUtterances()
} else {
  function loadOtherComment () {
    loadUtterances()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>